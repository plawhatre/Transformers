global:
  inp_dim:
    512
  d_model:
    512
  d_hidden:
    2048 #4 * d_model
  num_heads:
    8
  p_drop:
    0.1 
  Nx:
    6
    
training:
  batch_size:
    3
  epochs:
    2
  lang:
    "mr"
  sent_limit:
    100 # -1 represent no limit
  max_seq_len:
    100
  lr:
    1e-6
  beta1:
    0.9
  beta2:
    0.98
  eps:
    1e-9