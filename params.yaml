global:
  inp_dim:
    512
  d_model:
    512
  d_hidden:
    2048 #4 * d_model
  num_heads:
    8
  p_drop:
    0.1 
  Nx:
    1
    
training:
  flag:
    true
  save_model:
    true
  batch_size:
    64
  epochs:
    2
  lang:
    "mr"
  sent_limit:
    2000000 # -use wc -l loc/to/data/file and then decide on number
  max_seq_len:
    200
  lr:
    1e-3
  beta1:
    0.9
  beta2:
    0.98
  eps:
    1e-9
  save_path:
    './model'

inference:
  flag:
    true
  load_path:
    './model'
