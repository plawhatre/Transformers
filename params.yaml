global:
  inp_dim:
    512
  d_model:
    512
  d_hidden:
    2048 #4 * d_model
  num_heads:
    8
  p_drop:
    0.1 
  Nx:
    1
    
training:
  flag:
    true
  train_from_checkpoint_flag:
    false
  batch_size:
    32
  epochs:
    2
  lang:
    "mr"
  sent_limit:
    10000 # -use wc -l loc/to/data/file and then decide on number
  max_seq_len:
    150
  lr:
    1e-5
  beta1:
    0.9
  beta2:
    0.98
  eps:
    1e-9
  save_model:
    true
  save_path:
    './model'
  save_model_name:
    'final_model'
  checkpoint_filename_for_training:
    'final_model'
  checkpoint_interval:
    500

inference:
  flag:
    true
  load_path:
    './model'
  checkpoint_filename:
    'final_model'
